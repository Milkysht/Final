{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, date, time\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = final.copy()\n",
    "all_data.replace(\"-\", np.nan, inplace=True)\n",
    "clean_df = all_data.dropna(subset=['Transaction Date', 'Staff No.','Staff Name', 'Gender','Site',])\n",
    "grouped_df = clean_df.groupby([\"Transaction Date\", \"Staff No.\", \"Staff Name\", \"Gender\", \"Site\", \"Ranking\"]).sum().reset_index()\n",
    "melted_detail_df = pd.melt(grouped_df,\n",
    "                           id_vars=[\"Transaction Date\", \"Staff No.\", \"Staff Name\", \"Gender\", \"Site\", \"Ranking\"],\n",
    "                           value_name=\"Out\", var_name=\"Item Name\")\n",
    "melted_detail_df = melted_detail_df[melted_detail_df['Out'] != 0]\n",
    "\n",
    "cleaned_df = melted_detail_df.groupby([\"Transaction Date\", \"Staff No.\", \"Staff Name\", \"Gender\", \"Site\", \"Ranking\", \"Item Name\"])[\"Out\"].sum().reset_index()\n",
    "\n",
    "def is_float(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "a_float = cleaned_df['Out'].apply(is_float)\n",
    "\n",
    "cleaned_df = cleaned_df[a_float]\n",
    "cleaned_df['Out'] =cleaned_df['Out'].astype(float)\n",
    "\n",
    "split_columns = cleaned_df['Item Name'].str.split('-', expand=True)\n",
    "split_columns.columns = [\"Code\", \"Category\", \"Item\",\"size\", \"test\", \"test2\",\"test3\"]\n",
    "split_columns[\"Category\"] = split_columns[\"Category\"].str.replace('\\n', ' ')\n",
    "split_columns['Item_name'] = split_columns['Item'].astype(str) + \" \" + split_columns['size'].astype(str)\n",
    "split_columns = split_columns.drop([\"Item\",\t\"size\",\t\"test\",\"test2\",\t\"test3\"], axis=1)\n",
    "\n",
    "final_df = pd.concat(([split_columns, (cleaned_df.drop(['Item Name', 'Staff Name'],axis =1))]), axis=1)\n",
    "extra_date = final_df[final_df['Transaction Date']=='0/1/1900'].index\n",
    "final_df.drop(extra_date, inplace =True)\n",
    "final_df.rename(columns ={'Staff No.':'Staff_Number', 'Transaction Date': 'Transaction_Date'},inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"Uniform\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "spark_df = spark.createDataFrame(final_df)\n",
    "spark_df.printSchema()\n",
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.write.mode(\"overwrite\").saveAsTable(\"milky.consolidated_uniform\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.11.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
